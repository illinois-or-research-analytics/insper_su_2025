from pathlib import Path
import time
from typing import Iterator
import pandas as pd
import os
import click
from cpp_script import find_experiment_root, extract_memory_requested_from_sbatch, \
count_of_nodes_seeds_and_agents, count_number_of_superstars, in_degree_distribution,\
extract_run_time_from_err, extract_memory_used_from_err, extract_date_of_last_modification
import networkit as nk

def Extract_clustering_coefficient(edge_list: Path) -> dict:
    """
    Reads the edge list file and calculates the clustering coefficient
    using Networkit. Returns a dictionary with the GCC and ALCC values.
    Note: This function assumes the edge list is in CSV format with columns
    '#source' and 'target as the edgelist generated by the abm script'.
    """
    edges = pd.read_csv(edge_list)
    edges = edges.drop(columns=["year"])
    first_col = edges.columns[0]
    second_col = edges.columns[1]
    # Rename columns to acept any edgelist format:
    edges = edges.rename(columns={first_col: '#source', second_col: 'target'})
    
    min_source = edges["#source"].min()
    min_target = edges["target"].min()

    min_id = min_source if min_source < min_target else min_target

    temp_path = Path("temp_edgelist_for_networkit.csv")
    edges.to_csv(temp_path, index=False)

    try:
        reader = nk.graphio.EdgeListReader(
                separator=",",
                firstNode=min_id,
                commentPrefix="#",
                continuous=False,
                directed=False
            )
        G = reader.read(str(temp_path))
        cluster_coeff = nk.globals.ClusteringCoefficient()

        return {"gcc":cluster_coeff.exactGlobal(G),
                "alcc": cluster_coeff.sequentialAvgLocal(G)}
    
    finally:
        if temp_path.exists():
            os.remove(temp_path)

def extract_num_cycles_from_base_config(base_config: Path) -> int:
    """
    Extracts the number of cycles from the base configuration file.
    """
    with open(base_config, "r") as f:
        for line in f:
            if "num_years" in line:
                parts = line.split("=")
                return int(parts[1].strip())
    raise ValueError(f"'num_years' not found in {base_config}")

def extract_growth_rate_from_base_config(base_config: Path) -> float:
    """
    Extracts the growth rate from the base configuration file.
    """
    with open(base_config, "r") as f:
        for line in f:
            if "growth_rate" in line:
                parts = line.split("=")
                return float(parts[1].strip())
    raise ValueError(f"'growth_rate' not found in {base_config}")

def extract_alpha_from_base_config(base_config: Path) -> float:
    """
    Extracts the alpha value from the base configuration file.
    """
    with open(base_config, "r") as f:
        for line in f:
            if "alpha" in line:
                parts = line.split("=")
                return float(parts[1].strip())
    raise ValueError(f"'alpha' not found in {base_config}")

def extract_node_input_from_base_config(base_config: Path) -> str:
    """
    Extracts the node input file from the base configuration file.
    """
    with open(base_config, "r") as f:
        for line in f:
            if "cluster_data" in line:
                parts = line.split("/")
                node_input = parts[-1].strip()
                return node_input
    raise ValueError(f"'cluster_data' not found in {base_config}")

def extract_edge_input_from_base_config(base_config: Path) -> str:
    """
    Extracts the edge input file from the base configuration file.
    """
    with open(base_config, "r") as f:
        for line in f:
            if "edge_list" in line:
                parts = line.split("/")
                edge_input = parts[-1].strip()
                return edge_input
    raise ValueError(f"'edge_list' not found in {base_config}")

def extract_out_degree_from_base_config(base_config: Path) -> int:
    """
    Extracts the out degree from the base configuration file.
    """
    with open(base_config, "r") as f:
        for line in f:
            if "reference_count_table" in line:
                parts = line.split("/")
                out_degree = parts[-1].strip()
                return out_degree
    raise ValueError(f"'reference_count_table' not found in {base_config}")

def extract_agent_background_from_base_config(base_config: Path) -> str:
    """"
    Extracts the agent environment from the base configuration file.
    """
    flags = {"preferential_weight":False,
                  "recency_weight":False,
                  "fitness_weight":False
            }
    
    with open(base_config, "r") as f:
        for line in f:
            if "preferential_weight" in line:
                flags["preferential_weight"] = True
            elif "recency_weight" in line:
                flags["recency_weight"] = True
            elif "fitness_weight" in line:
                flags["fitness_weight"] = True
    if flags["preferential_weight"] and flags["recency_weight"] and flags["fitness_weight"]:
        return "sa"
    return "ra"

def generate_metrics(abm_err: Path, output_aux: Path, submit_job_sbatch: Path, edge_list: Path, base_config: Path) -> dict:
    """
    Main function to extract various metrics from the provided files.
    Returns a dictionary with the following and generates a csv file.
    """
    experiment_root = find_experiment_root(Path(abm_err).resolve().parent)
    dict_in_degree_distribution = in_degree_distribution(output_aux)
    dict_caunt_of_nodes = count_of_nodes_seeds_and_agents(output_aux)
    clustering_coeff = Extract_clustering_coefficient(edge_list)
    metrics = {
        "name_of_experiment": experiment_root.name,
        "date_of_experiment": extract_date_of_last_modification(abm_err),
        "input_nodelist": extract_node_input_from_base_config(base_config),
        "input_edgelist": extract_edge_input_from_base_config(base_config),
        "count_of_seeds": dict_caunt_of_nodes.get("seeds"),
        "count_of_agents": dict_caunt_of_nodes.get("agents"),
        "count_of_nodes": dict_caunt_of_nodes.get("total"),
        "num_cycles": extract_num_cycles_from_base_config(base_config),
        "growth_rate": extract_growth_rate_from_base_config(base_config),
        "agent_environment": extract_agent_background_from_base_config(base_config),
        "alpha": extract_alpha_from_base_config(base_config),
        "GCC": clustering_coeff.get("gcc"),
        "ALCC": clustering_coeff.get("alcc"),
        "count_of_superstars": count_number_of_superstars(output_aux),
        "seed_indeg_min": dict_in_degree_distribution.get("seed").get("min"),
        "seed_indeg_Q1": dict_in_degree_distribution.get("seed").get("Q1"),
        "seed_indeg_median": dict_in_degree_distribution.get("seed").get("median"),
        "seed_indeg_Q3": dict_in_degree_distribution.get("seed").get("Q3"),
        "seed_indeg_max": dict_in_degree_distribution.get("seed").get("max"),
        "agent_indeg_min": dict_in_degree_distribution.get("agent").get("min"),
        "agent_indeg_Q1": dict_in_degree_distribution.get("agent").get("Q1"),
        "agent_indeg_median": dict_in_degree_distribution.get("agent").get("median"),
        "agent_indeg_Q3": dict_in_degree_distribution.get("agent").get("Q3"),
        "agent_indeg_max": dict_in_degree_distribution.get("agent").get("max"),
        "out_degree_distribution": extract_out_degree_from_base_config(base_config),
        "run_time": extract_run_time_from_err(abm_err),
        "memory_requested": extract_memory_requested_from_sbatch(submit_job_sbatch),
        "memory_used": extract_memory_used_from_err(abm_err),
    }
    return metrics

def extract_metrics_V2(abm_err: Path, output_aux: Path, submit_job_sbatch: Path, 
                edge_list: Path, base_config: Path, df_metrics: pd.DataFrame = None) -> pd.DataFrame:
    """
    Runs the second version of the cassette, extractig metrics from the files provided by function:
    main_V2 and returns a pd.DataFrame with the metrics.
    """
    metrics = generate_metrics(abm_err, output_aux, submit_job_sbatch, edge_list, base_config)

    if df_metrics is None:
        df_metrics = pd.DataFrame([metrics])
        return df_metrics

    name = metrics["name_of_experiment"]
    df_metrics = df_metrics[df_metrics["name_of_experiment"] != name]

    df_metrics = df_metrics.reset_index(drop=True)

    df_metrics.loc[len(df_metrics)] = metrics

    return df_metrics

def discover_experiments(root: Path) -> Iterator[Path]:
    """
    Recursively walk the directory tree under `root` and yield every folder
    that looks like a complete ABM experiment, i.e. it contains all two
    subdirectories AND each of those contains the required file.

    An “experiment folder” must have:
      • errors/abm.err
      • submit_job.sbatch
      • output/random_timeline_0.csv
      • output/output.aux
      • base_config

    Yields:
        Path to each experiment directory found anywhere under `root`.
    """
    root = Path(root)
    for dirpath, dirnames, filenames in os.walk(root):
        d = Path(dirpath)

        # check that all three sub-folders exist
        if {"errors", "output"}.issubset(dirnames):
            # now verify the four required files inside them
            err_file   = d / "errors"  / "abm.err"
            sbatch_file= d / "submit_job.sbatch"
            aux_file   = d / "output"  / "output.aux"
            edge_list  = d / "output"  / "random_timeline_0.csv"
            base_config = d / "base_config"
            if err_file.exists() and edge_list.exists()\
               and sbatch_file.exists() and aux_file.exists()\
               and base_config.exists():
                yield d
            else:
                print(f"Skipping {d.name} not indentified as a complete experiment folder")
                print(f"{d.name} Missing one or more files: 'abm.err', 'edge_list' " \
                      "'submit_job.sbatch', 'output.aux', 'base_config'\n")
                
@click.command(help=
    """
    Recursively evaluate ABM experiment folders and update a master CSV.

    Usage:

    python_script.py --abm-outputs .../abm_outputs [--csv-path PATH]

    Options:

    --abm-outputs -> PATH Path to the abm_outputs directory (obligatory).

    --csv-path -> PATH CSV file to read/write metrics (default: .../abm_outputs/experiment_metrics.csv).
    """
    )

@click.option(
    "--abm-outputs", "abm_outputs",
    required=True,
    type=click.Path(exists=True, dir_okay=True, file_okay=False),
    help="-> Path to the abm_outputs directory containing experiment folders" \
    ""
)
@click.option(
    "--csv-path", "csv_path",
    required=False,
    type=click.Path(exists=False, dir_okay=False, file_okay=True, writable=True),
    help="-> Path to the master CSV file for metrics. If not provided," \
    "it will default to 'experiment_metrics.csv' in the abm_outputs directory." \
    ""
)
def cli(abm_outputs: Path, csv_path: Path = None):

    abm_outputs = Path(abm_outputs)

    metrics_df = None
    if csv_path:
        csv_path = Path(csv_path)
    else:
        csv_path = abm_outputs / "experiment_metrics.csv"

    if csv_path.exists():
        metrics_df = pd.read_csv(csv_path)
        processed = set(metrics_df["name_of_experiment"])
    else:
        metrics_df = None
        processed = set()

    evaluated_exp = []
    for exp_folder in discover_experiments(abm_outputs):

        if exp_folder.name in processed:
            print(f"Skipping already processed experiment: {exp_folder.name}\n")
            continue

        print(f"Evaluating experiment: {exp_folder.name}\n")

        abm_err           = exp_folder / "errors"  / "abm.err"
        output_aux        = exp_folder / "output"  / "output.aux"
        submit_job_sbatch = exp_folder / "submit_job.sbatch"
        edge_list         = exp_folder / "output"  / "random_timeline_0.csv"
        base_config       = exp_folder / "base_config"

        metrics_df = extract_metrics_V2(
            abm_err=abm_err,
            output_aux=output_aux,
            submit_job_sbatch=submit_job_sbatch,
            edge_list=edge_list,
            base_config=base_config,
            df_metrics=metrics_df
        )
        evaluated_exp.append(exp_folder.name)
    print(f"Evaluated experiments: {evaluated_exp}\n")

    if evaluated_exp:
        print(f"{csv_path.name} file saved to {csv_path.parent}")
        metrics_df.to_csv(csv_path, index=False)
    else:
        print("No new experiments to evaluate or no experiments found. No changes made to the CSV file.")

if __name__ == "__main__":
    cli()
