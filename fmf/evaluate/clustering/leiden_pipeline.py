import pandas as pd
import igraph as ig
import leidenalg as la
import numpy as np
from pathlib import Path
import click
import networkit as nk
from pymincut.pygraph import PyGraph

def nodeCoverage(outaux: Path, clustering: Path) -> float:
    df_aux = pd.read_csv(outaux, low_memory=False)
    df_clustering = pd.read_csv(clustering, low_memory=False)
    df_clustering = df_clustering[df_clustering["cluster_id"].duplicated(keep=False)]
    total_nodes = df_aux["node_id"].nunique()
    clustered_nodes = df_clustering["node_id"].nunique()
    return clustered_nodes/total_nodes

def clusteringDistribution(clustering: Path) -> dict:
    df_clustering = pd.read_csv(clustering, low_memory=False)
    total_clusters = df_clustering["cluster_id"].nunique()
    df_clustering = df_clustering[df_clustering["cluster_id"].duplicated(keep=False)]
    non_singleton_clusters = df_clustering["cluster_id"].nunique()
    cluster_sizes = df_clustering["cluster_id"].value_counts().values
    distribution = {
            "min"       : int(cluster_sizes.min()),
            "q1"        : float(np.percentile(cluster_sizes, 25)),
            "median"    : float(np.percentile(cluster_sizes, 50)),
            "q3"        : float(np.percentile(cluster_sizes, 75)),
            "p90"       : float(np.percentile(cluster_sizes, 90)),
            "max"       : int(cluster_sizes.max()),
            "singletons": int(total_clusters-non_singleton_clusters)
            }
    return distribution

def Extract_clustering_coefficient(edge_list: Path) -> dict:
    """
    Reads the edge list file and calculates the clustering coefficient
    using Networkit. Returns a dictionary with the GCC and ALCC values.
    Note: This function assumes the edge list is in CSV format with columns
    '#source' and 'target as the edgelist generated by the abm script'.
    """
    edges = pd.read_csv(edge_list)

    min_source = edges["#source"].min()
    min_target = edges["target"].min()

    min_id = min_source if min_source < min_target else min_target

    reader = nk.graphio.EdgeListReader(
            separator=",",
            firstNode=min_id,
            commentPrefix="#",
            continuous=False,
            directed=False
        )
    G = reader.read(str(edge_list))
    cluster_coeff = nk.globals.ClusteringCoefficient()

    return {"gcc":cluster_coeff.exactGlobal(G),
            "alcc": cluster_coeff.sequentialAvgLocal(G)}

def generateClusteringFile(experiment: Path, gamma: float, clustering_name: str) -> None:
    df = pd.read_csv(f"{experiment}/output/output.edgelist", dtype=str, low_memory=False)
    g = ig.Graph.TupleList(df.itertuples(index=False), directed=False, vertex_name_attr="name")
    partition = la.find_partition(g, la.CPMVertexPartition, resolution_parameter=gamma, seed=123456, n_iterations=2)
    df2 = pd.DataFrame({'node_id': [g.vs[i]["name"] for i in g.vs.indices], 'cluster_id': partition.membership})
    path_out = experiment / f"output/{clustering_name}"
    df2.to_csv(path_out, index=False)
    print(f"[DEBUG] Saved clustering to: {path_out.resolve()}")


def detect_delimiter(file_path):
    with open(file_path, 'r') as f:
        first_line = f.readline()
        if ',' in first_line:
            return ','
        elif '\t' in first_line:
            return '\t'
        elif ' ' in first_line:
            return ' '
        else:
            raise ValueError("No delimiter found in the first line of the file.")

def compute_mS_cS(neighbors, com):
    """
    Compute both the number of edges within the community (mS)
    and the number of edges on the boundary of the community (cS).
    Returns (mS, cS).
    """
    m_count = 0
    c_count = 0
    for node in com:
        for neighbor in neighbors.get(node, []):
            if neighbor in com:
                m_count += 1
            else:
                c_count += 1
    return m_count // 2, c_count

def compute_mS(neighbors, com):
    return compute_mS_cS(neighbors, com)[0]

def compute_cS(neighbors, com):
    return compute_mS_cS(neighbors, com)[1]

def compute_nS(com):
    """Compute the number of nodes in the community."""
    return len(com)
 
def compute_normalized_density(neighbors, com, m=None, n=None):
    """Compute the normalized density of the community."""
    m = compute_mS(neighbors, com) if m is None else m
    n = compute_nS(com) if n is None else n
    return 2 * m / (n * (n - 1)) if m > 0 else 0.0

def compute_mincut(neighbors, com):
    """Compute the mincut of the community."""
    cluster_edges = set()
    for node in com:
        for neighbor in neighbors.get(node, []):
            if neighbor in com:
                cluster_edges.add((node, neighbor))
    cluster_nodes = list(com)
    cluster_edges = list(cluster_edges)
    sub_G = PyGraph(cluster_nodes, cluster_edges)
    return sub_G.mincut("noi", "bqueue", False)[2]

def superstar_clusters(outaux: Path, clustering: Path, clustering_metrics: Path)-> dict:
    df_aux = pd.read_csv(outaux, low_memory=False)
    df_clustering = pd.read_csv(clustering, low_memory=False)
    df_metrics = pd.read_csv(clustering_metrics, low_memory=False)

    df_ss = (
        df_aux[df_aux["fit_peak_value"] > 1000]
        [["node_id", "fit_peak_value"]]
        .rename(columns={"fit_peak_value": "ss_fitness"})
    )
    
    if df_ss.empty:
        return None
    
    df_ss = df_ss.merge(
        df_clustering,
        on="node_id",
        how="left"
    )
    df_ss = df_ss.merge(
        df_metrics,
        on="cluster_id",
        how="left"
    )
    dict_superstar = {
        "ss_id":               df_ss["node_id"].tolist(),
        "ss_fit":              df_ss["ss_fitness"].tolist(),
        "cluster_id":          df_ss["cluster_id"].tolist(),
        "size":                df_ss["size"].tolist(),
        "intra_edges":         df_ss["intra_edges"].tolist(),
        "boundary_edges":      df_ss["boundary_edges"].tolist(),
        "normalized_density":  df_ss["normalized_density"].tolist(),
        "mincut":              df_ss["mincut"].tolist(),
    }
    return dict_superstar

def get_metrics(outaux, clustering_metrics, leiden_clustering) -> dict:
    df_outaux = pd.read_csv(outaux)
    df_outaux = df_outaux.set_index('node_id') 

    df_clustering_metrics = clustering_metrics.copy()
    df_leiden_clustering = pd.read_csv(leiden_clustering)

    metrics = {}
    cluster_to_nodes = df_leiden_clustering.groupby('cluster_id')['node_id'].apply(list).to_dict()
    cluster_ids = df_clustering_metrics['cluster_id'].unique()
    for cluster_id in cluster_ids:
        recency_w = 0
        pa_w= 0
        fitness_w = 0
        fitness = 0
        seeds = 0
        agents = 0

        node_ids = cluster_to_nodes.get(cluster_id, [])

        for node_id in node_ids:
            if node_id in df_outaux.index:
                row = df_outaux.loc[node_id]
                if row['type'] == 'seed':
                    seeds += 1
                else:
                    agents += 1
                    recency_w += row['rec_weight']
                    pa_w += row['pa_weight']
                    fitness_w += row['fit_weight']
                fitness += row['fit_peak_value']
        
        if agents == 0: #cluster with only seeds
            avg_recency_w = None
            avg_pa_w = None
            avg_fitness_w = None
        else:
            avg_recency_w = f"{float(recency_w / agents):.4f}"
            avg_pa_w = f"{float(pa_w / agents):.4f}"
            avg_fitness_w = f"{float(fitness_w / agents):.4f}"

        size = len(node_ids)
        avg_fitness = f"{float(fitness / size):.4f}"
            

        metrics[cluster_id] = {
            'avg_recency_w': avg_recency_w,
            'avg_pa_w': avg_pa_w,
            'avg_fitness_w': avg_fitness_w,
            'avg_fitness': avg_fitness,
            'seeds': int(seeds),
            'agents': int(agents)
        }



    metrics_df = pd.DataFrame.from_dict(metrics, orient='index')
    metrics_df.index.name = 'cluster_id'  
    df_clustering_metrics = df_clustering_metrics.merge(metrics_df, on='cluster_id', how='left')
    return df_clustering_metrics

def explode_superstars(df_ss: pd.DataFrame) -> pd.DataFrame:
    explode_cols = [
        'ss_id', 'ss_fit', 'cluster_id', 'size',
        'intra_edges', 'boundary_edges',
        'normalized_density', 'mincut'
    ]
    for col in explode_cols:
        df_ss = df_ss.explode(col).reset_index(drop=True)
    return df_ss

@click.command()
@click.option('--experiments-dirs', required=True, multiple=True,
              type=click.Path(exists=True, path_type=Path),
              help='experiments dirs')
@click.option('--output', required=True, type=click.Path(path_type=Path),
              help='Path to save final csv')
@click.option('--ss-out', required=True, type=click.Path(path_type=Path),
              help='Path to save superstar clusters CSV file')
@click.option('--full-pipeline', is_flag=True, default=False,
              help='Run the full pipeline including clustering generation')
@click.option('--clustering-name', default="leidenClustering.csv",
              help='Name of the clustering file to be generated')
@click.option('--metrics-name', default="clustering_metrics.csv",
              help='Name of the metrics file to be generated')
@click.option('--gamma', default=0.001, type=float,
              help='Resolution parameter for clustering algorithm')
def cli(experiments_dirs, output, ss_out, full_pipeline=False, clustering_name="leidenClustering.csv", metrics_name="clustering_metrics.csv", gamma=0.001):
    all_metrics = []
    all_ss = []
    for exp_dir in experiments_dirs:
        if full_pipeline:
            generateClusteringFile(exp_dir, gamma, clustering_name)
        outaux_path = exp_dir / "output/output.aux"
        clustering_path = exp_dir / f"output/{clustering_name}"
        edgelist_path = exp_dir / "output/output.edgelist"

        metrics = clusteringDistribution(clustering_path)
        metrics["node_coverage"] = nodeCoverage(outaux_path, clustering_path)
        parts = str(exp_dir).split("/")
        metrics["exp_name"] = parts[-1]
        gcc_lcc = Extract_clustering_coefficient(edgelist_path)
        metrics["gcc"] = gcc_lcc["gcc"]
        metrics["alcc"] = gcc_lcc["alcc"]

        #min
        delimiter = detect_delimiter(clustering_path)
        node_id2iid = {}
        node_iid2id = {}
        node_iid_count = 0

        com_id2iid = {}
        com_iid2id = {}
        com_iid_count = 0

        node2coms = {}
        com2nodes = {}

        with open(clustering_path, 'r') as f:
            lines = f.readlines()
            for line in lines:
                if line.startswith('#') or line.startswith('node_id'):
                    continue
                
                parts = line.strip().split(delimiter)
                assert len(parts) == 2, "Each line in the community file should contain exactly two parts: node ID and community ID."

                node_id = parts[0]
                if node_id not in node_id2iid:
                    node_id2iid[node_id] = node_iid_count
                    node_iid2id[node_iid_count] = node_id
                    node_iid_count += 1
                node_iid = node_id2iid[node_id]

                com_id = parts[1]
                if com_id not in com_id2iid:
                    com_id2iid[com_id] = com_iid_count
                    com_iid2id[com_iid_count] = com_id
                    com_iid_count += 1
                com_iid = com_id2iid[com_id]

                node2coms.setdefault(node_iid, set()).add(com_iid)
                com2nodes.setdefault(com_iid, set()).add(node_iid)

        delimiter = detect_delimiter(edgelist_path)

        neighbors = {}
        outliers = set()

        with open(edgelist_path, 'r') as f:
            lines = f.readlines()
            for line in lines:
                if line.startswith('#'):
                    continue

                parts = line.strip().split(delimiter)
                assert len(parts) == 2, "Each line in the network file should contain exactly two nodes."

                node1, node2 = parts[0], parts[1]

                if node1 not in node_id2iid:
                    node_iid = node_iid_count
                    outliers.add(node_iid)
                    node_id2iid[node1] = node_iid
                    node_iid2id[node_iid] = node1
                    node_iid_count += 1

                if node2 not in node_id2iid:
                    node_iid = node_iid_count
                    outliers.add(node_iid)
                    node_id2iid[node2] = node_iid
                    node_iid2id[node_iid] = node2
                    node_iid_count += 1

                node1_iid = node_id2iid[node1]
                node2_iid = node_id2iid[node2]

                neighbors.setdefault(node1_iid, set()).add(node2_iid)
                neighbors.setdefault(node2_iid, set()).add(node1_iid)
        rows = []
        for com_iid in range(com_iid_count):

            assert com_iid in com2nodes, f"Community IID {com_iid} not found in com2nodes mapping."
            com = com2nodes[com_iid]

            rows.append({
                "cluster_id":         com_iid2id[com_iid],
                "size":               compute_nS(com),
                "intra_edges":        compute_mS(neighbors, com),
                "boundary_edges":     compute_cS(neighbors, com),
                "normalized_density": compute_normalized_density(neighbors, com),
                "mincut":             compute_mincut(neighbors, com),
            })

        df_temp = pd.DataFrame(rows, columns=[
            "cluster_id",
            "size",
            "intra_edges",
            "boundary_edges",
            "normalized_density",
            "mincut"
        ])
        df_temp['cluster_id'] = df_temp['cluster_id'].astype(int)
        df_clust = get_metrics(outaux_path, df_temp, clustering_path)
        out_path = exp_dir / "output" / f"{metrics_name}"
        df_clust.to_csv(out_path, index=False)
        print(f"[DEBUG] clustering metrics written to {out_path}")
        #end min
        ss_clusters = superstar_clusters(outaux_path, clustering_path, out_path)
        if ss_clusters is not None:
            parts = str(exp_dir).split("/")
            ss_clusters['exp_name'] = parts[-1]
            ss_clusters['gamma'] = gamma
            all_ss.append(ss_clusters)
        all_metrics.append(metrics)

    df = pd.DataFrame(all_metrics)
    df = df[["exp_name", "node_coverage", "min", "q1", "median", "q3", "p90", "max", "singletons", "gcc", "alcc"]]
    df.to_csv(output, index=False)
    if len(all_ss) > 0:
            df_ss_clusters = pd.DataFrame(all_ss)
            df_ss_clusters = explode_superstars(df_ss_clusters)
            g = str(gamma).replace('.', '_')
            df_ss_clusters.to_csv(ss_out, index=False)
            print(f"[DEBUG] Superstar clusters written to {ss_out}")
    print(f"Summary written to {output}")

if __name__ == "__main__":
    cli()